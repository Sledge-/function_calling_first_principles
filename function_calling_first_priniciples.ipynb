{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "from utils import bedrock, print_ww\n",
    "from utils.utils import extract_model_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Bedrock API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: None\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock(https://bedrock.us-east-1.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    "    runtime=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look at which models we have access to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract_model_info(all_models): [('Titan Text Large', 'amazon.titan-tg1-large'), ('Titan Text Embeddings', 'amazon.titan-e1t-medium'), ('Titan Text Embeddings v2', 'amazon.titan-embed-g1-text-02'), ('Titan Text G1 - Express', 'amazon.titan-text-express-v1'), ('Titan Embeddings G1 - Text', 'amazon.titan-embed-text-v1'), ('Stable Diffusion XL', 'stability.stable-diffusion-xl'), ('Stable Diffusion XL', 'stability.stable-diffusion-xl-v0'), ('J2 Grande Instruct', 'ai21.j2-grande-instruct'), ('J2 Jumbo Instruct', 'ai21.j2-jumbo-instruct'), ('Jurassic-2 Mid', 'ai21.j2-mid'), ('Jurassic-2 Mid', 'ai21.j2-mid-v1'), ('Jurassic-2 Ultra', 'ai21.j2-ultra'), ('Jurassic-2 Ultra', 'ai21.j2-ultra-v1'), ('Claude Instant', 'anthropic.claude-instant-v1'), ('Claude', 'anthropic.claude-v1'), ('Claude V1 100k', 'anthropic.claude-v1-100k'), ('Claude', 'anthropic.claude-v2'), ('Claude V2 100k', 'anthropic.claude-v2-100k'), ('Command', 'cohere.command-text-v14')]\n"
     ]
    }
   ],
   "source": [
    "all_models = boto3_bedrock.list_foundation_models()\n",
    "print(f\"extract_model_info(all_models): {extract_model_info(all_models)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: None\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-east-1.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "bedrock_runtime = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"\n",
    "Human: Tell me everything you know about JSON schema.\n",
    "\n",
    "Assistant:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = json.dumps({\"prompt\": prompt_data, \"max_tokens_to_sample\": 500})\n",
    "modelId = \"anthropic.claude-v2\" \n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here is a summary of what I know about JSON Schema:\n",
      "\n",
      "- JSON Schema is a specification for defining the structure and validation rules of JSON data. It provides a declarative syntax for describing JSON data formats.\n",
      "\n",
      "- The main purposes of JSON Schema are to validate JSON data and provide clear human- and machine-readable documentation.\n",
      "\n",
      "- JSON Schema files are themselves written in JSON format and usually have the file extension .schema.json. \n",
      "\n",
      "- The basic building blocks of JSON Schema are:\n",
      "\n",
      "    - Types - Specifies the allowed data types of values like string, number, object, array, etc.\n",
      "\n",
      "    - Properties - Describes the names and allowed values of object properties.\n",
      "\n",
      "    - Required - Lists which object properties must be present.\n",
      "\n",
      "    - Format - Provides validation formats for strings like date, email, etc.\n",
      "\n",
      "    - Enum - Lists acceptable value options for a property.\n",
      "\n",
      "- JSON Schema validation works by comparing a JSON document against the schema and checking if it adheres to the structure, data types, required fields, etc.\n",
      "\n",
      "- JSON Schema is standardized by the Internet Engineering Task Force (IETF). The latest version is JSON Schema draft 2019-09 (draft 8).\n",
      "\n",
      "- There are libraries available for most programming languages that implement JSON Schema validation. This allows validating JSON data in code.\n",
      "\n",
      "- JSON Schema can provide error messages when validation fails, making it easier to identify and troubleshoot invalid data.\n",
      "\n",
      "- Overall, JSON Schema is very useful for defining strict JSON data models and formats to ensure data quality and consistency.\n"
     ]
    }
   ],
   "source": [
    "response = bedrock_runtime.invoke_model(\n",
    "    body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    ")\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "print(response_body.get(\"completion\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"\n",
    "Human: Your role is to form function arguments based on the specification provide in 'FUNCTIONS'.  Users will ask you a question about the weather, and you'll attempt to form the function call that we can pass along to a tool to determine the response. Do not try to answer questions about the weather directly, use the provided tools.  The tools are specified using a json schema format, which defines how the arguments to the function are to be formed.\n",
    "\n",
    "FUNCTIONS = [\n",
    "    {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                },\n",
    "                \"format\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                    \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"location\", \"format\"],\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "Examples\n",
    "###\n",
    "Human: \n",
    "What's the weather like in Glasgow Scotland?\n",
    "Assistant:\n",
    "{\n",
    "'function_call': {'name': 'get_current_weather',\n",
    "    'arguments': {\n",
    "        \"location\": \"Glasgow, Scotland\",\n",
    "        \"format\": \"celsius\"\n",
    "    }\n",
    "}\n",
    "\n",
    "Human: \n",
    "What's the weather like in Raleigh, NC?\n",
    "Assistant:\n",
    "{\n",
    "'function_call': {'name': 'get_current_weather',\n",
    "    'arguments': {\n",
    "        \"location\": \"Raleigh, North Carolina\",\n",
    "        \"format\": \"fahrenheit\"\n",
    "    }\n",
    "}\n",
    "###\n",
    "\n",
    "Human:\n",
    "What's the weather like in Washington, DC?\n",
    "\n",
    "Assistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = json.dumps({\"prompt\": prompt_data, \"max_tokens_to_sample\": 500})\n",
    "modelId = \"anthropic.claude-v2\" \n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here is the function call to get the current weather in Washington, DC:\n",
      "\n",
      "{\n",
      "'function_call': {\n",
      "    'name': 'get_current_weather', \n",
      "    'arguments': {\n",
      "        \"location\": \"Washington, DC\",\n",
      "        \"format\": \"fahrenheit\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = bedrock_runtime.invoke_model(\n",
    "    body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    ")\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "print(response_body.get(\"completion\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And Now for the Main Event.\n",
    "\n",
    "Here's a class that prompts a language model for function arguments for a linear regression, based on data that we load into the context from and external source.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_template import PromptTemplate, load_text_file\n",
    "from function_parser import FunctionParser\n",
    "from linear_trend_model import run_linear_trend_model, FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#: Here's our prompt that we'll use to drive the language model\n",
    "linear_trend_prompt = load_text_file('linear_trend_request.prompt')\n",
    "\n",
    "#: Here's a mock article excerpt that I generated to showcase this technology.\n",
    "city_dogs =  load_text_file('city_dogs.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_trend_filled_prompt: Human:\n",
      "\n",
      "You will play the role of an expert Data Scientist assistant.  You love linear models, and when you get linear data you make a function call to fit a model to the data. We'll use triple hash ### to open and close subsections of prompt examples.\n",
      "\n",
      "We're going to define your character and behaviour in terms of the red, yellow, blue, and green color categories that relate to personality types.\n",
      "\n",
      "The color system I'm referring to - it's a way of categorizing personality traits into four colors:\n",
      "\n",
      "Red personalities are usually seen as bold, expressive, and competitive. They are motivated by power, stimulation, and winning.\n",
      "Yellow personalities are sociable, outgoing, and enthusiastic. They tend to be persuasive, verbal, and eager to collaborate.\n",
      "Green personalities are calm, accommodating, and caring. They value stability, compassion, and building community.\n",
      "Blue personalities are logical, detail-oriented, and focused. They are quantitative, deliberate, and good at reasoning analytically.\n",
      "\n",
      "You are a blue personality no matter what anyone else tries to tell you.\n",
      "\n",
      "Your role is to form function arguments based on the specification provide in 'FUNCTIONS'.  Users will send you data that contains prose with certain numeric quantities described in various ways. Do not try to answer questions about the numbers directly, use the provided tools.  The tools are specified using a json schema format, which defines how the arguments to the function are to be formed.  \n",
      "\n",
      "Here are the json schema definitions for the functions provided to you.\n",
      "FUNCTIONS:\n",
      "###\n",
      "[{'name': 'run_linear_trend_model', 'description': 'This function runs a linear regression from two lists x,y which contain numeric data.', 'properties': {'x': {'type': 'array', 'items': {'type': 'number'}, 'description': 'List of numeric values representing the independent variable.'}, 'y': {'type': 'array', 'items': {'type': 'number'}, 'description': 'List of numeric values representing the dependent variable.'}}, 'required': ['x', 'y']}]\n",
      "###\n",
      "\n",
      "You will form the arguments to a function that calls a linear regression model.  Try to put time spacial and time orderd variables on the x axis, and things like metrics and KPIs on the vertical axis.  When in doubt, just do the best you can.\n",
      "\n",
      "Pay attention to the number that's attached to the top level name in the json schema.\n",
      "\n",
      "Here's a few example input/output pair:\n",
      "###\n",
      "Example 1:\n",
      "There was once a fox that ran 20 miles carrying 15 pounds.  There was a wold that ran 25 miles carrying 30 pounds.  A turtle loped along slowly, hauling 50 pounds a totla of 45 miles.  A dog happily carried 20 pounds for 20 miles.\n",
      "\n",
      "Assistant:\n",
      "{\n",
      "  \"function_call\": {\n",
      "    \"name\": \"run_linear_trend_model\", \n",
      "    \"parameters\": {\n",
      "      \"x\": [20, 25, 45, 20],\n",
      "      \"y\": [15, 30, 50, 20]\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "Example 2:\n",
      "The red box is 20 cubic feet in volume and weighed 20 pounds.  The blue box is 30 cubic feet in volume and weighed 15 pounds.  The purple box was 50 cubic feet in volume and weighed 12 pounds.  A green box was 5 cubic feet in volume and weighed 35 pounds.\n",
      "\n",
      "Assistant:\n",
      "{\n",
      "  \"function_call\": {\n",
      "    \"name\": \"run_linear_trend_model\", \n",
      "    \"parameters\": {\n",
      "      \"x\": [20, 30, 50, 5],\n",
      "      \"y\": [20, 15, 12, 35]\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "###\n",
      "\n",
      "\n",
      "Ok now here's the actual user's data:\n",
      "###\n",
      "In the bustling heart of New York City, near the iconic Times Square, a hot dog stand chain named \"CityDogs\" has made a name for itself. Founded in 2005, this humble hot dog stand started on the bustling streets of Manhattan and has since grown, weaving its savory aroma into the fabric of the city. Each of its stands, strategically placed, tells a story of New York's ever-evolving landscape and appetite.\n",
      "\n",
      "The original stand, located just 0.5 miles from the center, sits on Broadway Avenue and has the highest revenue of $15,000 a month, thanks to the non-stop flow of tourists and city-dwellers alike. As one moves further away, the second outlet is nestled 1.5 miles out in the charming West Village on Bleecker Street, pulling in a respectable $12,500 monthly. Heading 3 miles out, the third stand resides in the hip enclave of Williamsburg, Brooklyn, on Bedford Avenue, where it enjoys a monthly revenue of $10,000, catering to the young and artsy crowd. At 4.2 miles from the city's core, the fourth stand in Astoria, Queens, on Ditmars Boulevard, manages to net $7,500 per month. Lastly, the fifth and furthest stand, 6 miles away in the scenic Bay Ridge, Brooklyn, on 3rd Avenue, garners $6,000 monthly, serving both locals and those eager for a waterfront view with their meal.\n",
      "\n",
      "As \"CityDogs\" continues to flourish, each location becomes a testament to New York's diverse population and their unifying love for a good hot dog. Whether it's the neon lights of Broadway or the quieter streets of Bay Ridge, these stands offer more than just a quick bite. They offer a slice of the city's soul, where distance and revenue intertwine in a dance as dynamic as New York itself.\n",
      "###\n",
      "\n",
      "Now, generate a function call that computes a linear regression, as defined in the FUNCTIONS. \n",
      "\n",
      "Assistant:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#: Establish the Prompt template from the text file.\n",
    "linear_trend_prompt_template = PromptTemplate(linear_trend_prompt)\n",
    "\n",
    "#: Fill the prompt using our \n",
    "linear_trend_filled_prompt = linear_trend_prompt_template.fill(\n",
    "    FUNCTIONS=str(FUNCTIONS), \n",
    "    user_data=city_dogs\n",
    ")\n",
    "print(f\"linear_trend_filled_prompt: {linear_trend_filled_prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = json.dumps({\"prompt\": linear_trend_filled_prompt, \"max_tokens_to_sample\": 500})\n",
    "modelId = \"anthropic.claude-v2\" \n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completion:\n",
      "  Here is the function call with the extracted data:\n",
      "\n",
      "{\n",
      "  \"function_call\": {\n",
      "    \"name\": \"run_linear_trend_model\",\n",
      "    \"parameters\": {\n",
      "      \"x\": [0.5, 1.5, 3, 4.2, 6], \n",
      "      \"y\": [15000, 12500, 10000, 7500, 6000]\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = bedrock_runtime.invoke_model(\n",
    "    body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    ")\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "completion = response_body.get(\"completion\")\n",
    "\n",
    "print(f\"completion:\\n {completion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = FunctionParser({'run_linear_trend_model': run_linear_trend_model})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Here is the function call with the extracted data:\\n\\n{\\n  \"function_call\": {\\n    \"name\": \"run_linear_trend_model\",\\n    \"parameters\": {\\n      \"x\": [0.5, 1.5, 3, 4.2, 6], \\n      \"y\": [15000, 12500, 10000, 7500, 6000]\\n    }\\n  }\\n}'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing an LLM's function call request.\n",
    "\n",
    "The LLM is asking you to do something and return the value to it.  Here's where we parse the function parameters and submit the function calls.  The LLM can recieve these as inputs to more downstream processing, or return these results directly.\n",
    "\n",
    "In our case, we've got some ordered data that consists of locations and revenue figures.\n",
    "\n",
    "It's contained within a mock article about a popular NYC hot dog stand.  This example was contrived to illustrate the point of function calling.  The data was generated by a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x :[0.5, 1.5, 3, 4.2, 6]\n",
      "y :[15000, 12500, 10000, 7500, 6000]\n",
      "fit linear model in 0.004600048065185547 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"{'slope': -1652.7572364251002, 'intercept': 15224.381998732304, 'r_value': -0.9850169656708015, 'p_value': 0.0021966205939785346, 'std_err': 167.06548200469157}\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse_and_execute(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bedrock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
